{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d5a269-2062-49d5-bc95-3874d8cbae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/.local/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a60435-9b15-44b9-9590-e6d87fb271c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the downsampling layers (contracting path)\n",
    "        self.downs = nn.ModuleList()\n",
    "\n",
    "        # Define the upsampling layers (expanding path)\n",
    "        self.ups_transpose = nn.ModuleList()  # List for ConvTranspose2d layers\n",
    "        self.ups_conv = nn.ModuleList()       # List for DoubleConv blocks\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down path of U-Net\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up path of U-Net (upsampling)\n",
    "        for feature in reversed(features):\n",
    "            self.ups_transpose.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups_conv.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        # Final 1x1 convolution to get the output channels\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def down(self, x, skip_connections):\n",
    "        \"\"\"\n",
    "        This method performs the downsampling (contracting) part of the U-Net.\n",
    "        It appends the feature map to skip_connections and pools the output.\n",
    "        \"\"\"\n",
    "        for down in self.downs:\n",
    "            x = down(x)  # Apply DoubleConv block\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)  # Apply max-pooling for downsampling\n",
    "        return x\n",
    "\n",
    "    def up(self, x, skip_connections):\n",
    "        \"\"\"\n",
    "        This method performs the upsampling (expanding) part of the U-Net.\n",
    "        It applies ConvTranspose2d followed by DoubleConv at each step.\n",
    "        \"\"\"\n",
    "        skip_connections = skip_connections[::-1]  # Reverse the skip connections list\n",
    "        for idx in range(len(self.ups_transpose)):  # Loop through the transpose layers\n",
    "            x = self.ups_transpose[idx](x)  # Apply ConvTranspose2d to upsample\n",
    "            skip_connection = skip_connections[idx]  # Get the corresponding skip connection\n",
    "\n",
    "            # Resize the upsampled output to match the skip connection shape\n",
    "            #(this will occur if the image pixels aren't divided by 2)\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # Concatenate the skip connection with the upsampled feature map\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "\n",
    "            # Apply DoubleConv block to refine the concatenated feature map\n",
    "            x = self.ups_conv[idx](concat_skip)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        x = self.down(x, skip_connections)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.up(x, skip_connections)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c088a66a-354f-450f-a899-6858502f3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Step 3: Set dataset paths using current working directory\n",
    "current_dir = os.getcwd()  # Get current working directory (useful for Jupyter Notebooks)\n",
    "train_images_dir = os.path.join(current_dir, \"train\")\n",
    "train_masks_dir = os.path.join(current_dir, \"train_masks\")\n",
    "verification_images_dir = os.path.join(current_dir, \"validation\")\n",
    "verification_masks_dir = os.path.join(current_dir, \"validation_masks\")\n",
    "\n",
    "def get_loaders(\n",
    "    train_images_dir,\n",
    "    train_masks_dir,\n",
    "    verification_images_dir,\n",
    "    verification_masks_dir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    # Create train dataset\n",
    "    train_ds = CarvanaDataset(\n",
    "        image_dir=train_images_dir,       # Use the updated variable for train images\n",
    "        mask_dir=train_masks_dir,         # Use the updated variable for train masks\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    # Create train DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Create validation dataset\n",
    "    val_ds = CarvanaDataset(\n",
    "        image_dir=verification_images_dir,   # Use the updated variable for validation images\n",
    "        mask_dir=verification_masks_dir,     # Use the updated variable for validation masks\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    # Create validation DataLoader\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60eb4502-2969-457b-b5c3-80890c80de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "batch_size = 4\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define the transformation for validation\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "    train_images_dir,          # Path to training images\n",
    "    train_masks_dir,           # Path to training masks\n",
    "    verification_images_dir,   # Path to validation images\n",
    "    verification_masks_dir,    # Path to validation masks\n",
    "    batch_size,\n",
    "    train_transform,           # Training transformations\n",
    "    val_transforms,            # Validation transformations\n",
    "    NUM_WORKERS,\n",
    "    PIN_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a3d237-0be0-480f-a818-ce019cdd25d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (downs): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ups_transpose): ModuleList(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (ups_conv): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pretrained Model\n",
    "model = torch.load(\n",
    "    'unet_complete_model.pth',\n",
    "    map_location=device,\n",
    "    weights_only=False  # Required if model contains custom classes\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44900a8-ba4c-48ca-aa3c-78b511ed5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, delta=None):\n",
    "    dice_scores = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            if delta is not None:\n",
    "                images = torch.clamp(images + delta, 0, 1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Apply sigmoid and threshold\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            preds = (preds > 0.5).float()\n",
    "            \n",
    "            # Reshape masks to match preds: (B, H, W) -> (B, 1, H, W)\n",
    "            masks = masks.unsqueeze(1)\n",
    "            \n",
    "            dice = dice_score(preds, masks)\n",
    "            dice_scores.append(dice.item())\n",
    "    \n",
    "    model.train()\n",
    "    return sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    smooth = 1e-5\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c207571b-8e66-4b4e-92f5-eec7c14830ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool_attack(model, image, mask, num_classes=2, max_iter=50, overshoot=0.02, device='cuda'):\n",
    "    \"\"\"\n",
    "    DeepFool attack implementation for segmentation models\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.clone().detach().to(device)\n",
    "    mask = mask.clone().detach().to(device)\n",
    "    \n",
    "    batch_size, _, H, W = image.shape\n",
    "    pert_image = image.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Initialize perturbation\n",
    "    r_total = torch.zeros_like(image).to(device)\n",
    "    loop_i = 0\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        while loop_i < max_iter:\n",
    "            pert_image.requires_grad = True\n",
    "            output = model(pert_image)\n",
    "            \n",
    "            # Convert mask to class indices (0 or 1)\n",
    "            target = (mask > 0.5).float()\n",
    "            \n",
    "            # Calculate current classification\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct = (pred == target).all()\n",
    "            if correct:\n",
    "                break\n",
    "                \n",
    "            # Compute gradients\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(output, target)\n",
    "            grad = torch.autograd.grad(loss, pert_image, retain_graph=False)[0]\n",
    "            \n",
    "            # Compute perturbation direction\n",
    "            w = grad / (grad.norm() + 1e-8)\n",
    "            r_i = (loss + 1e-4) * w\n",
    "            \n",
    "            # Accumulate perturbation\n",
    "            r_total = (r_total + r_i).clamp(-overshoot, overshoot)\n",
    "            pert_image = torch.clamp(image + r_total, 0, 1).detach()\n",
    "            \n",
    "            loop_i += 1\n",
    "            \n",
    "    return pert_image.detach(), r_total.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d4314c-9e71-41aa-bdc0-0c198562fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(image, pert_image, clean_pred, pert_pred, mask, save_dir, filename):\n",
    "    \"\"\"Plot and save attack results with proper perturbation scaling\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Convert tensors to numpy arrays\n",
    "    image_np = image.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    pert_image_np = pert_image.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    perturbation_np = pert_image_np - image_np  # Range [-0.02, 0.02]\n",
    "    \n",
    "    # Normalize perturbation for visualization\n",
    "    max_val = np.abs(perturbation_np).max()\n",
    "    perturbation_normalized = (perturbation_np + max_val) / (2 * max_val)  # [0, 1]\n",
    "    \n",
    "    # Get predictions\n",
    "    clean_pred_np = clean_pred.detach().cpu().squeeze().numpy()\n",
    "    pert_pred_np = pert_pred.detach().cpu().squeeze().numpy()\n",
    "    mask_np = mask.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    # Plot images\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title('Original Image')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(pert_image_np)\n",
    "    plt.title('Perturbed Image')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(perturbation_normalized, cmap='coolwarm', vmin=0, vmax=1)\n",
    "    plt.title('Perturbation (Normalized)')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(clean_pred_np, cmap='gray')\n",
    "    plt.title('Clean Prediction')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(pert_pred_np, cmap='gray')\n",
    "    plt.title('Perturbed Prediction')\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(mask_np, cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345f44a3-7c8b-4937-a853-a3a534c58609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deepfool_attack(model, val_loader, save_dir='deepfool_attack_results', device='cuda'):\n",
    "    \"\"\"Run DeepFool attack on validation set\"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    dice_scores = []\n",
    "    num_samples = 0\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(val_loader)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).unsqueeze(1)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        pert_images, _ = deepfool_attack(model, images, masks, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get predictions\n",
    "            clean_outputs = model(images)\n",
    "            clean_preds = (torch.sigmoid(clean_outputs) > 0.5).float()\n",
    "            \n",
    "            pert_outputs = model(pert_images)\n",
    "            pert_preds = (torch.sigmoid(pert_outputs) > 0.5).float()\n",
    "            \n",
    "            # Calculate Dice score\n",
    "            dice = dice_score(pert_preds, masks)\n",
    "            dice_scores.append(dice.item())\n",
    "            \n",
    "        # Save visualizations for first 5 samples\n",
    "        for i in range(min(5, images.size(0))):\n",
    "            plot_and_save_results(\n",
    "                image=images[i],\n",
    "                pert_image=pert_images[i],\n",
    "                clean_pred=clean_preds[i],\n",
    "                pert_pred=pert_preds[i],\n",
    "                mask=masks[i],\n",
    "                save_dir=save_dir,\n",
    "                filename=f'batch_{batch_idx}_sample_{i}.png'\n",
    "            )\n",
    "            \n",
    "        num_samples += images.size(0)\n",
    "        \n",
    "    # Calculate final metrics\n",
    "    avg_dice = np.mean(dice_scores)\n",
    "    print(f'\\nDeepFool Attack Results:')\n",
    "    print(f'Average Dice Score: {avg_dice:.4f}')\n",
    "    \n",
    "    return avg_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53bc3372-27aa-4bb2-b1a9-3665b8f820c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 12/12 [03:48<00:00, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DeepFool Attack Results:\n",
      "Average Dice Score: 0.6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Clean Dice: 0.9837\n",
      "DeepFool Dice: 0.6758\n",
      "Performance Drop: 0.3079\n"
     ]
    }
   ],
   "source": [
    "# Run DeepFool attack\n",
    "attack_dice = run_deepfool_attack(model, val_loader)\n",
    "\n",
    "# Compare with clean performance\n",
    "clean_dice = evaluate(val_loader)\n",
    "print(f'\\nPerformance Comparison:')\n",
    "print(f'Clean Dice: {clean_dice:.4f}')\n",
    "print(f'DeepFool Dice: {attack_dice:.4f}')\n",
    "print(f'Performance Drop: {clean_dice - attack_dice:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffa8f9-05a6-438d-96f5-6f2b5637ab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
