{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIj9WVT4yj8S",
    "outputId": "3934d9b5-ea84-400c-b592-d0fa39b60c6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/.local/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B4qT7JNZynR8"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vW3cN38q8Lgp"
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the downsampling layers (contracting path)\n",
    "        self.downs = nn.ModuleList()\n",
    "\n",
    "        # Define the upsampling layers (expanding path)\n",
    "        self.ups_transpose = nn.ModuleList()  # List for ConvTranspose2d layers\n",
    "        self.ups_conv = nn.ModuleList()       # List for DoubleConv blocks\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down path of U-Net\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up path of U-Net (upsampling)\n",
    "        for feature in reversed(features):\n",
    "            self.ups_transpose.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups_conv.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        # Final 1x1 convolution to get the output channels\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def down(self, x, skip_connections):\n",
    "        \"\"\"\n",
    "        This method performs the downsampling (contracting) part of the U-Net.\n",
    "        It appends the feature map to skip_connections and pools the output.\n",
    "        \"\"\"\n",
    "        for down in self.downs:\n",
    "            x = down(x)  # Apply DoubleConv block\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)  # Apply max-pooling for downsampling\n",
    "        return x\n",
    "\n",
    "    def up(self, x, skip_connections):\n",
    "        \"\"\"\n",
    "        This method performs the upsampling (expanding) part of the U-Net.\n",
    "        It applies ConvTranspose2d followed by DoubleConv at each step.\n",
    "        \"\"\"\n",
    "        skip_connections = skip_connections[::-1]  # Reverse the skip connections list\n",
    "        for idx in range(len(self.ups_transpose)):  # Loop through the transpose layers\n",
    "            x = self.ups_transpose[idx](x)  # Apply ConvTranspose2d to upsample\n",
    "            skip_connection = skip_connections[idx]  # Get the corresponding skip connection\n",
    "\n",
    "            # Resize the upsampled output to match the skip connection shape\n",
    "            #(this will occur if the image pixels aren't divided by 2)\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # Concatenate the skip connection with the upsampled feature map\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "\n",
    "            # Apply DoubleConv block to refine the concatenated feature map\n",
    "            x = self.ups_conv[idx](concat_skip)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        x = self.down(x, skip_connections)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.up(x, skip_connections)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgef91sf-w3R"
   },
   "source": [
    "## Tesing the Model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJm0tHp59Dcn",
    "outputId": "3f12145b-1a46-464e-9b33-e6fb0c15a447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything looks fine\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 1, 161, 161))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "preds = model(x)\n",
    "if preds.shape == x.shape:\n",
    "  print(\"Everything looks fine\")\n",
    "else:\n",
    "  print(\"Go to sleep, debug tomorrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNBRxlkZDRqP"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m23qd1bQDVXz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 2: Define Dataset class\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Step 3: Set dataset paths using current working directory\n",
    "current_dir = os.getcwd()  # Get current working directory (useful for Jupyter Notebooks)\n",
    "train_images_dir = os.path.join(current_dir, \"train\")\n",
    "train_masks_dir = os.path.join(current_dir, \"train_masks\")\n",
    "verification_images_dir = os.path.join(current_dir, \"validation\")\n",
    "verification_masks_dir = os.path.join(current_dir, \"validation_masks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-sEeBEOG6p7E"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-inVQ7OH62i2"
   },
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    train_images_dir,\n",
    "    train_masks_dir,\n",
    "    verification_images_dir,\n",
    "    verification_masks_dir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    # Create train dataset\n",
    "    train_ds = CarvanaDataset(\n",
    "        image_dir=train_images_dir,       # Use the updated variable for train images\n",
    "        mask_dir=train_masks_dir,         # Use the updated variable for train masks\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    # Create train DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Create validation dataset\n",
    "    val_ds = CarvanaDataset(\n",
    "        image_dir=verification_images_dir,   # Use the updated variable for validation images\n",
    "        mask_dir=verification_masks_dir,     # Use the updated variable for validation masks\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    # Create validation DataLoader\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ovBbdF4r7Xar"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9YChlxu07dNs"
   },
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"./save_image\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure the directory exists on Google Drive\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "        # Save predictions and ground truth as images\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/gt_{idx}.png\")  # Save ground truth\n",
    "\n",
    "    model.train()\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        #forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O2DX-zD94axx"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 4\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "KfFn54wi5n8S",
    "outputId": "dc3c9628-e91a-46ae-881d-ad115d0e3ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7936816/38400000 with acc 20.67\n",
      "Dice score: 0.342151939868927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 511/511 [04:35<00:00,  1.85it/s, loss=0.0781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 37845988/38400000 with acc 98.56\n",
      "Dice score: 0.9660236239433289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 511/511 [04:33<00:00,  1.87it/s, loss=0.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 38119995/38400000 with acc 99.27\n",
      "Dice score: 0.9824203252792358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 511/511 [04:33<00:00,  1.87it/s, loss=0.0368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 38056487/38400000 with acc 99.11\n",
      "Dice score: 0.9786209464073181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 511/511 [04:33<00:00,  1.87it/s, loss=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Got 37989163/38400000 with acc 98.93\n",
      "Dice score: 0.974643349647522\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation for training\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define the transformation for validation\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Load the data using the get_loaders function, which should accept the dataset paths\n",
    "train_loader, val_loader = get_loaders(\n",
    "    train_images_dir,          # Path to training images\n",
    "    train_masks_dir,           # Path to training masks\n",
    "    verification_images_dir,   # Path to validation images\n",
    "    verification_masks_dir,    # Path to validation masks\n",
    "    BATCH_SIZE,\n",
    "    train_transform,           # Training transformations\n",
    "    val_transforms,            # Validation transformations\n",
    "    NUM_WORKERS,\n",
    "    PIN_MEMORY,\n",
    ")\n",
    "\n",
    "# If LOAD_MODEL is set to True, load the model checkpoint\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "\n",
    "# Check accuracy on the validation set\n",
    "check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "    # Save model checkpoint after each epoch\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    # Check accuracy after each epoch\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "    # Save predictions as images after each epoch\n",
    "    save_predictions_as_imgs(\n",
    "      val_loader, model, folder=\"./save_images/\", device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bNun7sGcc7JX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Accuracy On Training Set\n",
      "Got 155770633/156940800 with acc 99.25\n",
      "Dice score: 0.9824142456054688\n",
      "Checking Accuracy On Validation Set\n",
      "Got 37989163/38400000 with acc 98.93\n",
      "Dice score: 0.974643349647522\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Accuracy On Training Set\")\n",
    "check_accuracy(train_loader, model, device=DEVICE)\n",
    "print(\"Checking Accuracy On Validation Set\")\n",
    "check_accuracy(val_loader, model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unet_state_dict.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'unet_complete_model_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
